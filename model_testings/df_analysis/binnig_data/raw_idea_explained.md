# Smart Binning for Industrial Data: Raw Idea

This document explains the ideas and implementation in `model_testings/df_analysis/binnig_data/raw_idea.py` for transforming continuous industrial signals into informative discrete bins. It covers the motivation, the four binning methods implemented, how binning is applied to new data, and how to assess binning quality. It also includes figures to illustrate distributions, bin boundaries, and quality metrics.

Figures are generated by the companion script `generate_raw_idea_figures.py` and saved under `model_testings/df_analysis/binnig_data/figures/`.

## Why binning?

Industrial signals (temperature, pressure, flow rate, etc.) are noisy, skewed, and often non-stationary. Binning (a.k.a. discretization):

- Splits continuous ranges into a small number of interpretable levels.
- Reduces noise by aggregating similar values.
- Enables rule-based reasoning and robust modeling (e.g., tree rules and contingency analysis).
- Makes feature interactions and regimes easier to visualize.

## Functions overview

File: `model_testings/df_analysis/binnig_data/raw_idea.py`

- `smart_binning(df, n_bins, method='quantile', target_col=None)`
  - Orchestrates binning for each numeric column using one of: `quantile`, `kmeans`, `entropy`, `equal_width`.
  - Returns `(binned_df, bin_info)` where `bin_info[col]['bins']` are the bin edges used.

- `_quantile_binning(series, n_bins)`
  - Equal-frequency bins using `pd.qcut`. Robust to skew. Falls back to equal-width when needed.

- `_kmeans_binning(series, n_bins)`
  - Unsupervised clustering with `KMeans`. Cluster centers are sorted; bin edges are midpoints between adjacent centers. Groups naturally similar values.

- `_entropy_binning(series, target, n_bins)` and `_find_optimal_entropy_bins(...)`
  - Supervised idea aiming to maximize information gain to a target. Current simplified approach starts from many quantile cuts and compresses to `n_bins` using approximate split locations. Good starting point; see Improvements below.

- `_equal_width_binning(series, n_bins)`
  - Divides numeric range into equal-length intervals. Simple baseline.

- `apply_binning_transform(df, bin_info)`
  - Applies previously computed bins to new data. Useful for production transforms and train/serve parity.

- `analyze_binning_quality(df, binned_df, bin_info)`
  - Computes quality metrics per feature: number of used bins, bin counts, a balance score, and an information retention score (correlation between original values and bin centers).

## Conceptual details

- Quantile (equal-frequency)
  - Objective: each bin contains roughly the same number of samples.
  - Pros: robust to skew/outliers; balanced classes.
  - Cons: unequal bin widths; edges can fall in dense/noisy areas.

- KMeans-based
  - Objective: group dense regions; boundaries at midpoints between learned centers.
  - Pros: adapts to multimodal distributions; interpretable centers.
  - Cons: random init sensitivity (mitigated here via `n_init=10`); less stable with very heavy tails.

- Entropy-based (supervised)
  - Objective: maximize information about a target, akin to decision-tree splits.
  - Pros: yields bins that are predictive of the target.
  - Cons: current implementation is a simplified heuristic. For production, consider MDLP, decision-tree discretization, or dynamic programming for optimal partitions.

- Equal-width
  - Objective: uniform bin width over numeric range.
  - Pros: simplest; easy to interpret.
  - Cons: poor with skewed distributions; bins may be highly imbalanced.

## Applying to new data

`apply_binning_transform(df, bin_info)` cuts new values using stored edges. Notes:

- Values outside the original min/max become NaN with `pd.cut`. Consider padding edges to `(-inf, +inf)` or expanding min/max when persisting bins to avoid NaNs on mild distribution drift.
- Use consistent `include_lowest=True` to cover the left edge.

## Quality metrics

`analyze_binning_quality(df, binned_df, bin_info)` returns for each feature:

- `unique_bins`: number of bins actually used.
- `bin_counts`: samples per bin.
- `balance_score`: 1 âˆ’ std(counts)/expected_count. Higher is better (more balanced bins). Expected count = N/unique_bins.
- `information_retention`: correlation between original values and bin centers mapped back to samples. Captures monotonic fidelity.

Interpretation:
- Quantile should score high on balance.
- KMeans/Entropy may improve information retention on multimodal or target-informative features.

## Figures

Run the generator script (see below). It will create:

- Distribution with bin edges for each feature and method
  - `figures/temperature_quantile_bins.png`
  - `figures/temperature_kmeans_bins.png`
  - `figures/temperature_entropy_bins.png`
  - `figures/temperature_equal_width_bins.png`
  - Similarly for `pressure_*.png` and `flow_rate_*.png`

- Bin count bars per feature/method
  - `figures/temperature_quantile_counts.png`, etc.

- Summary quality across methods per feature
  - `figures/temperature_info_retention_by_method.png`
  - `figures/temperature_balance_score_by_method.png`
  - Similarly for `pressure_*` and `flow_rate_*`

You can embed or view these files directly from the repo.

## How to reproduce figures

1) Install Python deps (typical scientific stack):
- pandas, numpy, matplotlib, seaborn, scikit-learn, scipy

2) Generate figures:

```bash
python model_testings/df_analysis/binnig_data/generate_raw_idea_figures.py
```

The script will:
- Create synthetic data (same distributions as the example in `raw_idea.py`).
- Run all four binning methods with `n_bins=5` and `target='target'` where applicable.
- Save figures under `model_testings/df_analysis/binnig_data/figures/`.

## Example usage (code)

```python
from raw_idea import smart_binning, analyze_binning_quality, apply_binning_transform

binned_df, bin_info = smart_binning(df, n_bins=5, method='quantile', target_col='target')
analysis = analyze_binning_quality(df, binned_df, bin_info)
new_df_binned = apply_binning_transform(new_df, bin_info)
```

## Limitations and improvements

- Entropy binning can be strengthened:
  - MDLP (Minimum Description Length Principle) discretization.
  - Greedy tree-based splits with pruning (CART-like), then merge to target `n_bins`.
  - Dynamic programming to maximize mutual information under bin-count constraint.
- Out-of-range handling for production transforms (pad with `-inf/+inf`).
- Stability checks across time to guard against drift.

## Takeaways

- Use quantile when you need balanced bins and robustness.
- Use KMeans for multimodal or clustered distributions.
- Use entropy when bins must be predictive of a known target.
- Evaluate with balance and information retention, not just visual intuition.
